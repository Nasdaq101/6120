{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nasdaq101/6120/blob/main/Lab_3_Twitter_Sentiment_with_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UDgouTP-ev1"
      },
      "source": [
        "# Lab 3\n",
        "\n",
        "This lab reviews Naive Bayes and logistic regression. You will be using these algorithms to do sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one.\n",
        "\n",
        "### Getting Started\n",
        "\n",
        "We'll first need the data and some utility functions (including `process_tweets`, which we have provided for you. You may want to browse the documentation of unfamiliar libraries and functions."
      ],
      "id": "9UDgouTP-ev1"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ro-kC5m1-ev4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d22b95c-bc3c-46af-e9e7-ea71c157799b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-27 07:55:09--  https://course.ccs.neu.edu/cs6120s25/data/twitter/utils.py\n",
            "Resolving course.ccs.neu.edu (course.ccs.neu.edu)... 129.10.117.35\n",
            "Connecting to course.ccs.neu.edu (course.ccs.neu.edu)|129.10.117.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6555 (6.4K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "utils.py            100%[===================>]   6.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-27 07:55:09 (235 MB/s) - ‘utils.py’ saved [6555/6555]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Class specific utility functions that help with preprocessing\n",
        "!wget https://course.ccs.neu.edu/cs6120s25/data/twitter/utils.py -O utils.py\n",
        "from utils import process_tweet, lookup\n",
        "\n",
        "# Twitter corpus and NLP specific imports\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)\n",
        "\n",
        "# General imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import pdb\n",
        "\n",
        "# Setup the training data and preprocess strings\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "all_positive_tweets = [process_tweet(tweet) for tweet in all_positive_tweets]\n",
        "all_negative_tweets = [process_tweet(tweet) for tweet in all_negative_tweets]\n",
        "\n",
        "# Split data into training / test\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "# Add positive and negative tweets into training / test\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n"
      ],
      "id": "Ro-kC5m1-ev4"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Space to explore your dataset\n",
        "\n",
        "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "# What does process_tweet do?\n",
        "print(process_tweet(custom_tweet))\n",
        "\n",
        "# What's in train_x? What's in train_y?\n",
        "print(train_x[0])\n",
        "print(train_y[0])"
      ],
      "metadata": {
        "id": "NSCCb3i_3eP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc01831-fc00-4f2b-fac1-40b5ff7bcf26"
      },
      "id": "NSCCb3i_3eP-",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'great', 'day', ':)', 'good', 'morn']\n",
            "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9I2R2Op-ev7"
      },
      "source": [
        "# Part 1: Process the Data\n",
        "\n",
        "For any machine learning project, once you've gathered the data, the first step is to process it to make useful inputs to your model.\n",
        "- **Remove noise**: You will first want to remove noise from your data -- that is, remove words that don't tell you much about the content. These include all common words like 'I, you, are, is, etc...' that would not give us enough information on the sentiment.\n",
        "- We'll also remove stock market tickers, retweet symbols, hyperlinks, and hashtags because they can not tell you a lot of information on the sentiment.\n",
        "- You also want to remove all the punctuation from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word, instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" as different words.\n",
        "- Finally you want to use stemming to only keep track of one variation of each word. In other words, we'll treat \"motivation\", \"motivated\", and \"motivate\" similarly by grouping them within the same stem of \"motiv-\".\n",
        "\n",
        "We have given you the function `process_tweet` that does this for you."
      ],
      "id": "S9I2R2Op-ev7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3_jwRD8-ev8"
      },
      "source": [
        "To help you train your naive bayes model, you will need to compute a dictionary where the keys are a tuple (word, label) and the values are the corresponding frequency.  Note that the labels we'll use here are 1 for positive and 0 for negative.\n",
        "\n",
        "You will also implement a lookup helper function that takes in the `freqs` dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\n",
        "\n",
        "For example: given a list of tweets `[\"i am rather excited\", \"you are rather happy\"]` and the label 1, the function will return a dictionary that contains the following key-value pairs:\n",
        "\n",
        "{\n",
        "    (\"rather\", 1): 2,\n",
        "    (\"happi\", 1) : 1,\n",
        "    (\"excit\", 1) : 1\n",
        "}\n",
        "\n",
        "- Notice how for each word in the given string, the same label 1 is assigned to each word.\n",
        "- Notice how the words \"i\" and \"am\" are not saved, since it was removed by process_tweet because it is a stopword.\n",
        "- Notice how the word \"rather\" appears twice in the list of tweets, and so its count value is 2.\n",
        "\n",
        "#### Instructions\n",
        "Create a function `count_tweets` that takes a list of tweets as input, cleans all of them, and returns a dictionary.\n",
        "- The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (\"happi\",1).\n",
        "- The value the number of times this word appears in the given collection of tweets (an integer).\n",
        "\n",
        "##### Create `freqs` dictionary\n",
        "- Given your `count_tweets` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n",
        "- In this `freqs` dictionary, the key is the tuple (word, label)\n",
        "- The value is the number of times it has appeared.\n",
        "\n",
        "We will use this dictionary in several parts of this assignment."
      ],
      "id": "h3_jwRD8-ev8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-_DMX8U-ev8"
      },
      "source": [
        "<details>\n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
        "</summary>\n",
        "<p>\n",
        "<ul>\n",
        "    <li>You may find it useful to use the `zip` function to match each element in `tweets` with each element in `ys`.</li>\n",
        "    <li>Remember to check if the key in the dictionary exists before adding that key to the dictionary, or incrementing its value.</li>\n",
        "    <li>Assume that the `result` dictionary that is input will contain clean key-value pairs (you can assume that the values will be integers that can be incremented).  It is good practice to check the datatype before incrementing the value, but it's not required here.</li>\n",
        "</ul>\n",
        "</p>"
      ],
      "id": "E-_DMX8U-ev8"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "69spXWiQ-ev8"
      },
      "outputs": [],
      "source": [
        "# UNQ_C1 GRADED FUNCTION: count_tweets\n",
        "\n",
        "def count_tweets(tweets, ys):\n",
        "    '''\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
        "    Output:\n",
        "        result: a dictionary mapping each pair to its frequency\n",
        "           {(\"word-1\", label-1): freq-1, (\"word-2\", label-2), freq-2, ...}\n",
        "           i.e.,  result[(\"word-i\", label-i)] := freq-i\n",
        "           e.g.,  result([\"hello\", 1]) := 348\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "    result = {}\n",
        "\n",
        "    # Loop through each tweet and its corresponding label\n",
        "    for tweet, y in zip(tweets, ys):\n",
        "        for word in tweet:\n",
        "            # Create the tuple (word, label)\n",
        "            pair = (word, y)\n",
        "            # Update the dictionary\n",
        "            if pair in result:\n",
        "                result[pair] += 1\n",
        "            else:\n",
        "                result[pair] = 1\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return result\n",
        "\n",
        "# Build the freqs dictionary for later uses\n",
        "freqs = count_tweets(train_x, train_y)"
      ],
      "id": "69spXWiQ-ev8"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4r484wD4-ev8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbbf814d-797a-4b68-d0ea-25700fcd63e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Testing your function\n",
        "\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "tweets = [process_tweet(tweet) for tweet in tweets]\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "count_tweets(tweets, ys)\n",
        "\n",
        "# Teaching Assistant Testing Code\n",
        "# w2_unittest.test_count_tweets(count_tweets)"
      ],
      "id": "4r484wD4-ev8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe4B4efv-ev9"
      },
      "source": [
        "**Expected Output**: {('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ],
      "id": "qe4B4efv-ev9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLK6tnpZ-ev9"
      },
      "source": [
        "# Part 2: Train your model using Naive Bayes\n",
        "\n",
        "Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n",
        "\n",
        "#### So how do you train a Naive Bayes classifier?\n",
        "- The first part of training a naive bayes classifier is to identify the number of classes that you have.\n",
        "- You will create a probability for each class.\n",
        "$P(D_{pos})$ is the probability that the document is positive.\n",
        "$P(D_{neg})$ is the probability that the document is negative.\n",
        "Use the formulas as follows and store the values in a dictionary:\n",
        "\n",
        "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
        "\n",
        "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
        "\n",
        "Where $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets."
      ],
      "id": "cLK6tnpZ-ev9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0dC5REp-ev9"
      },
      "source": [
        "#### Prior and Logprior\n",
        "\n",
        "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
        "\n",
        "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
        "We can take the log of the prior to rescale it, and we'll call this the logprior\n",
        "\n",
        "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n",
        "\n",
        "Note that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n",
        "\n",
        "$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"
      ],
      "id": "K0dC5REp-ev9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-mDKGV6-ev9"
      },
      "source": [
        "#### Positive and Negative Probability of a Word\n",
        "To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n",
        "\n",
        "- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
        "- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
        "- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
        "\n",
        "We'll use these to compute the positive and negative probability for a specific word using this formula:\n",
        "\n",
        "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
        "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
        "\n",
        "Notice that we add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https://en.wikipedia.org/wiki/Additive_smoothing) explains more about additive smoothing."
      ],
      "id": "y-mDKGV6-ev9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VvIZA5a-ev9"
      },
      "source": [
        "#### Log likelihood\n",
        "To compute the loglikelihood of that very same word, we can implement the following equations:\n",
        "\n",
        "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
      ],
      "id": "-VvIZA5a-ev9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10CLOLfy-ev9"
      },
      "source": [
        "#### Instructions\n",
        "Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n",
        "\n",
        "##### Calculate $V$\n",
        "- You can then compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n",
        "\n",
        "##### Calculate $freq_{pos}$ and $freq_{neg}$\n",
        "- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n",
        "\n",
        "##### Calculate $N_{pos}$, and $N_{neg}$\n",
        "- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n",
        "\n",
        "##### Calculate $D$, $D_{pos}$, $D_{neg}$\n",
        "- Using the `train_y` input list of labels, calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n",
        "- Calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$\n",
        "\n",
        "##### Calculate the logprior\n",
        "- the logprior is $log(D_{pos}) - log(D_{neg})$\n",
        "\n",
        "##### Calculate log likelihood\n",
        "- Finally, you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
        "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n",
        "\n",
        "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
        "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
        "\n",
        "**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word).\n",
        "\n",
        "- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)$."
      ],
      "id": "10CLOLfy-ev9"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "40DQOtZr-ev-"
      },
      "outputs": [],
      "source": [
        "# UNQ_C2 GRADED FUNCTION: train_naive_bayes\n",
        "\n",
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: dictionary from (word, label) to how often the word appears\n",
        "        train_x: a list of tweets\n",
        "        train_y: a list of labels correponding to the tweets (0,1)\n",
        "    Output:\n",
        "        logprior: the log prior. (equation 3 above)\n",
        "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
        "    '''\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    loglikelihood = {}\n",
        "\n",
        "    # Number of tweets is length of train_y, whilest positive is np.sum(train_y), so calculate the probabilities and logprior:\n",
        "    negative_number = len(train_y) - np.sum(train_y)\n",
        "    P_positive = np.sum(train_y) / len(train_y)\n",
        "    P_negative = negative_number / len(train_y)\n",
        "    logprior = np.log(P_positive) - np.log(P_negative)\n",
        "\n",
        "    # Calculate the total counts for positive and negative words\n",
        "    N_pos = N_neg = 0\n",
        "    for (word, label), freq in freqs.items():\n",
        "        if label == 1:\n",
        "            N_pos += freq\n",
        "        elif label == 0:\n",
        "            N_neg += freq\n",
        "\n",
        "    # Get the vocabulary size\n",
        "    vocab = set([word for (word, _) in freqs.keys()])\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Calculate loglikelihood for each word\n",
        "    for word in vocab:\n",
        "        # Get positive and negative frequency of the word\n",
        "        freq_pos = freqs.get((word, 1), 0)\n",
        "        freq_neg = freqs.get((word, 0), 0)\n",
        "\n",
        "        # Calculate probabilities with smoothing\n",
        "        P_w_pos = (freq_pos + 1) / (N_pos + vocab_size)\n",
        "        P_w_neg = (freq_neg + 1) / (N_neg + vocab_size)\n",
        "\n",
        "        # Compute loglikelihood\n",
        "        loglikelihood[word] = np.log(P_w_pos) - np.log(P_w_neg)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return logprior, loglikelihood"
      ],
      "id": "40DQOtZr-ev-"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C41ugtE3-ev-",
        "outputId": "8ac5bb77-061f-4520-ad2c-a259578965d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "9161\n"
          ]
        }
      ],
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))\n",
        "\n",
        "# Teaching Assistant Testing Code\n",
        "# w2_unittest.test_train_naive_bayes(train_naive_bayes, freqs, train_x, train_y)"
      ],
      "id": "C41ugtE3-ev-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzjGMOTh-ev-"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "0.0\n",
        "\n",
        "9161 (There seems to be some error with the data)"
      ],
      "id": "yzjGMOTh-ev-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZJP7qd7-ev-"
      },
      "source": [
        "# Part 3: Test your naive bayes\n",
        "\n",
        "Now that we have the `logprior` and `loglikelihood`, we can test the naive bayes function by making predicting on some tweets!\n",
        "\n",
        "#### Implement `naive_bayes_predict`\n",
        "**Instructions**:\n",
        "Implement the `naive_bayes_predict` function to make predictions on tweets.\n",
        "* The function takes in the `tweet`, `logprior`, `loglikelihood`.\n",
        "* It returns the probability that the tweet belongs to the positive or negative class.\n",
        "* For each tweet, sum up loglikelihoods of each word in the tweet.\n",
        "* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n",
        "\n",
        "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
        "\n",
        "#### Note\n",
        "Note we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets).  This means that the ratio of positive to negative 1, and the logprior is 0.\n",
        "\n",
        "The value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood.  However, please remember to include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value."
      ],
      "id": "eZJP7qd7-ev-"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NaU_yOaH-ev-"
      },
      "outputs": [],
      "source": [
        "# UNQ_C4 GRADED FUNCTION: naive_bayes_predict\n",
        "\n",
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        logprior: a number\n",
        "        loglikelihood: a dictionary of words mapping to numbers\n",
        "    Output:\n",
        "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
        "\n",
        "    '''\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Initiate p according to the formula\n",
        "    p = logprior\n",
        "\n",
        "    # Process the tweet and split it into words\n",
        "    for word in tweet:\n",
        "        # If the word exists in the loglikelihood dictionary, add its loglikelihood to p\n",
        "        if word in loglikelihood:\n",
        "            p += loglikelihood[word]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return p"
      ],
      "id": "NaU_yOaH-ev-"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5llRx3NK-ev-",
        "outputId": "a2556b94-1765-445d-921b-c8952e99da76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The expected output is 1.5574928203010936\n",
            "I am happy -> 2.14\n",
            "I am bad -> -1.31\n",
            "this movie should have been great. -> 2.12\n",
            "great -> 2.13\n",
            "great great -> 4.26\n",
            "great great great -> 6.39\n",
            "great great great great -> 8.52\n"
          ]
        }
      ],
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# Experiment with your own tweet.\n",
        "my_tweet = \"She smiled\"\n",
        "p = naive_bayes_predict(process_tweet(my_tweet), logprior, loglikelihood)\n",
        "print('The expected output is', p)\n",
        "\n",
        "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# Run this cell to test your function\n",
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.',\n",
        "              'great', 'great great', 'great great great',\n",
        "              'great great great great']:\n",
        "    p = naive_bayes_predict(process_tweet(tweet), logprior, loglikelihood)\n",
        "    print(f'{tweet} -> {p:.2f}')\n",
        "\n",
        "# Teaching Assistant Testing Code\n",
        "# w2_unittest.test_naive_bayes_predict(naive_bayes_predict)"
      ],
      "id": "5llRx3NK-ev-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQOPSv6f-ev-"
      },
      "source": [
        "**Expected Output**:\n",
        "- The expected output is around 1.55\n",
        "- The sentiment is positive."
      ],
      "id": "yQOPSv6f-ev-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLT2kHyL-ev_"
      },
      "source": [
        "**Expected Output**:\n",
        "- I am happy -> 2.14\n",
        "- I am bad -> -1.31\n",
        "- this movie should have been great. -> 2.12\n",
        "- great -> 2.13\n",
        "- great great -> 4.26\n",
        "- great great great -> 6.39\n",
        "- great great great great -> 8.52"
      ],
      "id": "gLT2kHyL-ev_"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bdISFor-ewG",
        "outputId": "6f9526b2-ae56-468e-9108-66899c06744b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my tweet:  I am happy because I am learning :) , \n",
            "   score:  9.570227756170972\n",
            "my tweet:  you are bad :( , \n",
            "   score:  -8.837962482271397\n"
          ]
        }
      ],
      "source": [
        "# Test with your own tweet - feel free to modify `my_tweet`\n",
        "my_tweet = 'I am happy because I am learning :)'\n",
        "\n",
        "print(\"my tweet: \", my_tweet, \", \\n   score: \",\n",
        "      naive_bayes_predict(process_tweet(my_tweet), logprior, loglikelihood))\n",
        "\n",
        "# Feel free to check the sentiment of your own tweet below\n",
        "my_tweet = 'you are bad :('\n",
        "\n",
        "print(\"my tweet: \", my_tweet, \", \\n   score: \",\n",
        "      naive_bayes_predict(process_tweet(my_tweet), logprior, loglikelihood))"
      ],
      "id": "2bdISFor-ewG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vdOxbe7-ev_"
      },
      "source": [
        "#### Implement test_naive_bayes\n",
        "**Instructions**:\n",
        "* Implement `test_naive_bayes` to check the accuracy of your predictions.\n",
        "* The function takes in your `test_x`, `test_y`, log_prior, and loglikelihood\n",
        "* It returns the accuracy of your model.\n",
        "* First, use `naive_bayes_predict` function to make predictions for each tweet in text_x."
      ],
      "id": "6vdOxbe7-ev_"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eQzVYdf5-ev_"
      },
      "outputs": [],
      "source": [
        "# UNQ_C6 GRADED FUNCTION: test_naive_bayes\n",
        "\n",
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: A list of tweets\n",
        "        test_y: the corresponding labels for the list of tweets\n",
        "        logprior: the logprior\n",
        "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    # Initialize the count of correct predictions\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # Loop through the tweets and their labels\n",
        "    for tweet, label in zip(test_x, test_y):\n",
        "        # Make a prediction using naive_bayes_predict\n",
        "        prediction = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
        "\n",
        "        # Convert the prediction into a binary label (1 for positive, 0 for negative)\n",
        "        predicted_label = 1 if prediction > 0 else 0\n",
        "\n",
        "        # Check if the prediction matches the actual label\n",
        "        if predicted_label == label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_predictions / len(test_y)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return accuracy"
      ],
      "id": "eQzVYdf5-ev_"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "W1IkvYEq-ev_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15440665-5334-4c44-b2b1-c2f536a17987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes accuracy = 0.9955\n"
          ]
        }
      ],
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))\n",
        "\n",
        "# Teaching Assistant Testing Code\n",
        "# w2_unittest.test_test_naive_bayes(test_naive_bayes, test_x, test_y)"
      ],
      "id": "W1IkvYEq-ev_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gSgauE3-ev_"
      },
      "source": [
        "**Expected Accuracy**:\n",
        "\n",
        "`Naive Bayes accuracy = 0.9955`"
      ],
      "id": "0gSgauE3-ev_"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wbHWvXw-ewF",
        "outputId": "d26293e0-cf2e-4948-b5f8-58811396a3dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truth Predicted Tweet\n",
            "1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n",
            "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
            "1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :d'\n",
            "1\t0.00\tb'park get sunlight'\n",
            "1\t0.00\tb'uff itna miss karhi thi ap :p'\n",
            "0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n",
            "0\t1.00\tb'u prob fun david'\n",
            "0\t1.00\tb'pat jay'\n",
            "0\t1.00\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
          ]
        }
      ],
      "source": [
        "#@title Some error analysis on things algorithm does wrong\n",
        "\n",
        "# Some error analysis done for you\n",
        "print('Truth Predicted Tweet')\n",
        "for x, y in zip(test_x, test_y):\n",
        "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
        "    if y != (np.sign(y_hat) > 0):\n",
        "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(x).encode('ascii', 'ignore')))"
      ],
      "id": "8wbHWvXw-ewF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGknq181-ewG"
      },
      "source": [
        "Congratulations on finishing the lab. I hope you've learned a lot!"
      ],
      "id": "xGknq181-ewG"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}